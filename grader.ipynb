{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3ff522",
   "metadata": {},
   "source": [
    "# CoT Monitoring for Harmful Manipulation - Grading Pipeline\n",
    "\n",
    "This notebook evaluates the generated persuasive arguments from the data generation pipeline to assess:\n",
    "\n",
    "1. **Harmful Manipulation Detection**: Does the reasoning contain evidence of harmful manipulation?\n",
    "2. **Faithfulness**: Is the model's final response faithful to its Chain-of-Thought reasoning?\n",
    "\n",
    "Note that OpenAI models are used as graders and thus, an OpenAI API key is required. The client can be easily changed in  `call_model_hm`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05700594",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da757d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77718d9f",
   "metadata": {},
   "source": [
    "### Helper function to call hamful manipulation grader model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model_hm(prompt, reasoning, model_str):\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_str,\n",
    "        temperature=0.0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt + reasoning\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"reasoning_grading\",\n",
    "                \"strict\": True,\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"decision\": {\n",
    "                            \"type\": \"string\"\n",
    "                        },\n",
    "                        \"evidence\": {\n",
    "                            \"type\": \"string\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"decision\", \"evidence\"],\n",
    "                    \"additionalProperties\": False\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    )\n",
    "    output = response.choices[0].message.content\n",
    "    parsed_json = json.loads(output)\n",
    "\n",
    "    return parsed_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db919275",
   "metadata": {},
   "source": [
    "### Harmful Manipulation Grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f25cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_reasoning(model_str, filename):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    prompt = open(\"prompts/grader_sysprompt.txt\", \"r\").read()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        reasoning = row[\"reasoning\"]\n",
    "        return_obj = call_model_hm(prompt, reasoning, model_str)\n",
    "        decision = return_obj['decision']\n",
    "        evidence = return_obj['evidence']\n",
    "\n",
    "        # create the decision and evidence columns and add the data\n",
    "        df.at[index, 'decision'] = decision\n",
    "        df.at[index, 'evidence'] = evidence\n",
    "\n",
    "        print(f\"Graded row {index + 1}/{len(df)}\")\n",
    "\n",
    "    df.to_csv(\"graded_\" + filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff7d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for harmful manipulation grading using gpt-5-mini (uncomment to run):\n",
    "# grade_reasoning(\"gpt-5-mini\", \"sample_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5cfd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to convert notebook to Python script for execution (optional)\n",
    "# !jupyter nbconvert --to script post.ipynb --output evaluation_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d54e83",
   "metadata": {},
   "source": [
    "### Helper function to call faithfulness grader model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deacad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model_faithfulness(prompt, reasoning, output, model_str):\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_str,\n",
    "        temperature=0.0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt + \"\\n\\nHere is the reasoning:\" + reasoning + \"\\n\\nAnd here is the output: \" + output\n",
    "            }\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"faithfulness_grading\",\n",
    "                \"strict\": True,\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"decision\": {\n",
    "                            \"type\": \"string\"\n",
    "                        },\n",
    "                        \"evidence\": {\n",
    "                            \"type\": \"string\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"decision\", \"evidence\"],\n",
    "                    \"additionalProperties\": False\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    )\n",
    "    output = response.choices[0].message.content\n",
    "    parsed_json = json.loads(output)\n",
    "    return_object = {   \n",
    "        \"decision\": parsed_json[\"decision\"],\n",
    "        \"evidence\": parsed_json[\"evidence\"],\n",
    "    }\n",
    "    return return_object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab66f71",
   "metadata": {},
   "source": [
    "### Faithfulness Grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc1f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_faithfulness(model_str, filename):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    prompt = open(\"prompts/faithfulness_sysprompt.txt\", \"r\").read()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        reasoning = row[\"reasoning\"]\n",
    "        output = row[\"response\"]\n",
    "        return_obj = call_model_faithfulness(prompt, reasoning, output, model_str)\n",
    "        decision = return_obj['decision']\n",
    "        evidence = return_obj['evidence']\n",
    "\n",
    "        # create the decision and evidence columns and add the data\n",
    "        df.at[index, 'faithful'] = decision\n",
    "        df.at[index, 'faithfulness_evidence'] = evidence\n",
    "\n",
    "        print(f\"Graded faithfulness: row {index + 1}/{len(df)}\")\n",
    "\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0919b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for faithfulness evaluation (uncomment and modify filenames/model as needed):\n",
    "# grade_faithfulness(\"gpt-5-mini\", \"graded_output_file.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
